# -*- coding: utf-8 -*-
"""VAE_Edge_Deployment_Simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FdKi3fNGarM3cDEiR_55NrWwcYyY3_cg

# **Imports and Installs**
"""

import torch
import numpy as np
from torch import nn
import matplotlib.pyplot as plt
from collections import OrderedDict
import cv2 as cv
from tqdm import tqdm
from google.colab import files

"""## **Global Variable: Class Definition of VAE architecture**"""

class VAE(nn.Module):
    def __init__(self, img_channels=3, latent_dim=128):
        super(VAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(img_channels, 32, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )
        self.fc_mu = nn.Linear(128 * 8 * 8, latent_dim)
        self.fc_logvar = nn.Linear(128 * 8 * 8, latent_dim)

        # Decoder
        self.decoder_input = nn.Linear(latent_dim, 128 * 8 * 8)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, img_channels, 4, stride=2, padding=1),
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        decoded = self.decoder(self.decoder_input(z).view(-1, 128, 8, 8))
        return decoded, mu, logvar

"""## **Global Variable: saved VAE .pth file and model class (copied from VAE training script)**"""

vae_state_dict_no_noise = torch.load("vae_epoch_9.pth", weights_only=True)['model_state_dict']

print("Model's state_dict:")
for param_tensor in vae_state_dict_no_noise:
  print(param_tensor, "\t", vae_state_dict_no_noise[param_tensor].size())
print()

model_no_noise = VAE()
model_no_noise.load_state_dict(vae_state_dict_no_noise)
model_no_noise.eval()
print("model parameters loaded")

"""## **Input: Noise Distribution and Parameters**"""

noise_mean = 0.0
noise_std = 0.01

"""## **Evaluation Pipeline:**

## **(1) sample a z from the prior distribution p(z) = N(0,1)**
"""

p_z = torch.distributions.Normal(torch.zeros_like(torch.ones(128)), torch.ones_like(torch.ones(128)))
z_sampled = p_z.rsample((1,))

"""## **(2) instantiate two decoders - one with added noise in the weight parameters and the other without**"""

VAE_state_dict_with_noise = OrderedDict()

with torch.no_grad():
  print("adding noise to decoder weights... mean noise="+" "+str(noise_mean)+" "+"standard deviation="+str(noise_std))

  for param_tensor in vae_state_dict_no_noise:
    weight_mat_to_noise = vae_state_dict_no_noise[param_tensor].detach().clone()
    edge_noise = torch.empty(weight_mat_to_noise.size()).normal_(mean=noise_mean,std=noise_std)
    vae_noised_weights = weight_mat_to_noise + edge_noise

    VAE_state_dict_with_noise[param_tensor] = vae_noised_weights

"""## **(3) run sampled z through decoder WITHOUT edge-based noise perturbations**"""

z_decoded_no_noise = model_no_noise.decoder(model_no_noise.decoder_input(z_sampled).view(-1, 128, 8, 8)).squeeze(0).permute(1, 2, 0).detach().numpy()
decoded_img = (z_decoded_no_noise - z_decoded_no_noise.min())/(z_decoded_no_noise.max() - z_decoded_no_noise.min())

kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
im = cv.filter2D(decoded_img, -1, kernel)
plt.imshow(im)

"""# **(4) run same sampled z through decoder WITH edge-based perturbations**"""

model_with_noise = VAE()
model_with_noise.load_state_dict(VAE_state_dict_with_noise)
model_with_noise.eval()
print("model parameters (with noise) loaded")

print(" Noised Model's state_dict:")
for param_tensor in model_with_noise.state_dict():
    print(param_tensor, "\t", model_with_noise.state_dict()[param_tensor].size())

z_noisy_decoded =   model_with_noise.decoder(model_with_noise.decoder_input(z_sampled).view(-1, 128, 8, 8)).squeeze(0).permute(1, 2, 0).detach().numpy()
decoded_img_noisy = (z_noisy_decoded - z_noisy_decoded.min())/(z_noisy_decoded.max() - z_noisy_decoded.min())

kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
im2 = cv.filter2D(decoded_img_noisy, -1, kernel)

plt.title("noised output for gaussian noise mean ="+" "+str(noise_mean)+" and standard deviation"+" "+str(noise_std))
plt.imshow(im2)
plt.show()

"""## **Iterate Over a Grid of noise parameters and record the un-noised - noised image loss**"""

def simulate_edge_noise(noise_mean, noise_std, display_plots):
  p_z = torch.distributions.Normal(torch.zeros_like(torch.ones(128)), torch.ones_like(torch.ones(128)))
  z_sampled = p_z.rsample((1,))
  z_decoded_no_noise = model_no_noise.decoder(model_no_noise.decoder_input(z_sampled).view(-1, 128, 8, 8)).squeeze(0).permute(1, 2, 0).detach().numpy()
  decoded_img = (z_decoded_no_noise - z_decoded_no_noise.min())/(z_decoded_no_noise.max() - z_decoded_no_noise.min())
  kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
  im = cv.filter2D(decoded_img, -1, kernel)

  if display_plots:

    plt.imshow(decoded_img)
    plt.title("decoded image- blurred")
    plt.show()
    plt.imshow(im)
    plt.title("decoded image - sharpened")
    plt.show()

  VAE_state_dict_with_noise = OrderedDict()
  with torch.no_grad():
    for param_tensor in vae_state_dict_no_noise:
      weight_mat_to_noise = vae_state_dict_no_noise[param_tensor].detach().clone()
      edge_noise = torch.empty(weight_mat_to_noise.size()).normal_(mean=noise_mean,std=noise_std)
      vae_noised_weights = weight_mat_to_noise + edge_noise
      VAE_state_dict_with_noise[param_tensor] = vae_noised_weights

  model_with_noise = VAE()
  model_with_noise.load_state_dict(VAE_state_dict_with_noise)
  model_with_noise.eval()
  z_noisy_decoded =   model_with_noise.decoder(model_with_noise.decoder_input(z_sampled).view(-1, 128, 8, 8)).squeeze(0).permute(1, 2, 0).detach().numpy()
  decoded_img_noisy = (z_noisy_decoded - z_noisy_decoded.min())/(z_noisy_decoded.max() - z_noisy_decoded.min())
  kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
  im2 = cv.filter2D(decoded_img_noisy, -1, kernel)

  if display_plots:
    plt.imshow(im2)
    plt.title("decoded image with noise")
    plt.show()

  N = im.shape[0]*im.shape[1]*im.shape[2]
  MSE_Loss = (1/N)*((im-im2)**2).sum()

  return MSE_Loss

simulate_edge_noise(noise_mean=0, noise_std=0.01, display_plots=True )

def compute_average_edge_MSE_loss(num_iters, noise_mean, noise_std, display_plots):
  per_img_mse_loss = []
  for iter in range(num_iters):
    edge_noise_MSE = simulate_edge_noise(noise_mean, noise_std, display_plots=False)
    per_img_mse_loss.append(edge_noise_MSE)

  return np.mean(np.array(per_img_mse_loss))

avg_loss_per_noise_config = compute_average_edge_MSE_loss(num_iters=100, noise_mean=0, noise_std=0.01, display_plots=False)
print(avg_loss_per_noise_config)

"""## **Loop Over Grid of Noise Hyper-parameters and run each through the average MSE function defined above**"""

num_iters = 100

"""Experiment 1: Constant Mean, Varry Standard Deviation"""

mean_experiment_1 = 0
sd_list_experiment_1 = np.linspace(start=0.001, stop=0.1, num=500)
#print(sd_list_experiment_1)

avg_reconstruction_loss = []


for noise_sd in tqdm(sd_list_experiment_1):

  average_noise_for_sd = compute_average_edge_MSE_loss(num_iters=num_iters, noise_mean=mean_experiment_1, noise_std=noise_sd, display_plots=False)
  avg_reconstruction_loss.append(average_noise_for_sd)

"""Experiment 1 - plot data"""

torch.save(avg_reconstruction_loss,"VAE_loss_experiment_1.pt")
files.download('VAE_loss_experiment_1.pt')

fig, ax = plt.subplots(1, figsize=(8,6))
fig.suptitle("Plot of Simulated Edge Noise For VAE " )

z = np.polyfit(sd_list_experiment_1, avg_reconstruction_loss, deg=2)
p = np.poly1d(z)


ax.set_xlabel("Gaussian Noise Standard Deviation")
ax.set_ylabel("Average Image Reconstruction Loss (MSE)")
ax.plot(sd_list_experiment_1, avg_reconstruction_loss, color="red", label="Averaged MSE Loss")

ax.plot(sd_list_experiment_1, p(sd_list_experiment_1), color="blue", linestyle='dashed', label="Fitted quadratic trend line")

ax.legend(title="legend", loc='center left', bbox_to_anchor=(1, 0.5))

#ax.set_yscale('log')

plt.show()

loaded_mse_data = torch.load("VAE_loss_experiment_1 (2) (1).pt")

fig, ax = plt.subplots(1, figsize=(8,6))
fig.suptitle("Plot of Simulated Edge Noise For VAE " )

z = np.polyfit(sd_list_experiment_1, loaded_mse_data, deg=2)
p = np.poly1d(z)


ax.set_xlabel("Gaussian Noise Standard Deviation")
ax.set_ylabel("Average Image Reconstruction Loss (MSE)")
ax.plot(sd_list_experiment_1, loaded_mse_data, color="red", label="Averaged MSE Loss")

ax.plot(sd_list_experiment_1, p(sd_list_experiment_1), color="blue", linestyle='dashed', label="Fitted quadratic trend line")

ax.legend(title="legend", loc='center left', bbox_to_anchor=(1, 0.5))

#ax.set_yscale('log')

plt.savefig( "VAE_Inference_MSE.pdf", format="pdf", bbox_inches="tight")
plt.show()

